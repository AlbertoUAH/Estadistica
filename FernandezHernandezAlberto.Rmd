---
title: "Práctica Estadística"
author: "Fernández Hernández, Alberto"
date: "02/12/2020"
output:
  pdf_document:
    toc: yes
  html_document:
    df_print: paged
---

\newpage
# Ejercicio 1
## Apartado 1
__Dado el siguiente conjunto de datos, obtener con R las diferentes medidas de centralización y dispersión estudiadas. Así mismo obtener el diagrama de caja y bigotes.__

Inicialmente, partimos de los siguientes valores de estatura, recogidos en un DataFrame formado por las columnas __alumnos__ y __estaturas__:

```{r datos.estatura}
datos.estatura <- data.frame(alumnos = c("Alumno1", "Alumno2", "Alumno3", "Alumno4", 
                             "Alumno5", "Alumno6", "Alumno7", "Alumno8", "Alumno9", 
                             "Alumno10", "Alumno11", "Alumno12", "Alumno13", "Alumno14", 
                             "Alumno15", "Alumno16", "Alumno17", "Alumno18", "Alumno19", 
                             "Alumno20", "Alumno21", "Alumno22", "Alumno23", "Alumno24", 
                             "Alumno25", "Alumno26", "Alumno27", "Alumno28", "Alumno29", 
                             "Alumno30"),
                             estatura = c(1.25, 1.28, 1.27, 1.21, 1.22, 1.29, 1.30, 1.24, 
                              1.27, 1.29, 1.23, 1.26, 1.30, 1.21, 1.28, 1.30, 1.22, 1.25, 
                              1.20, 1.28, 1.21, 1.29, 1.26, 1.22, 1.28, 1.27, 1.26, 1.23, 
                              1.22, 1.21))
```

Comencemos con las medidas de posicionamiento central:

1. __MEDIA ARITMÉTICA__, empleando la función _mean_ definida en R:

```{r media_artimetica}
# Media artimetica
media.aritmetica <- mean(datos.estatura[, "estatura"])
media.aritmetica
```

Con el cálculo de las medidas de dispersión comprobaremos si la media es representativa o no de la muestra. Por otro lado, aunque no corresponde con la muestra empleada, también podemos calcular la __media geométrica__, basado en el producto de cada valor, obteniendo finalmente su raíz n-ésima (siendo n el total de datos de la muestra). Dado que R no dispone de una función específica, mediante la función _Reduce_ calcularemos el productorio, elevando el resultado a $\frac{1}{n}$:

```{r media_geometrica}
# Media geometrica
# Podemos ver que la Media geometrica es ligeramente inferior a la aritmetica
media.geometrica <- Reduce(prod, datos.estatura["estatura"], init = 1) ** 
                            (1/nrow(datos.estatura))
media.geometrica
```

2. __MEDIANA__, empleando la función _median_ definida en R:

```{r mediana}
mediana <- median(datos.estatura[, "estatura"])
mediana
```

Es decir, por debajo 1.26 metros se encuentra el 50 % de los alumnos de la muestra y por encima el 50 % restante.

3. __MODA__. Por desgracia, R no dispone de una función específica para el cálculo de la moda. Para ello, mediante la función _table_ creamos una tabla con las frecuencias absolutas de cada estatura:

```{r frecuencias_estaturas}
frecuencias.estaturas <- as.data.frame(table(Estatura = datos.estatura[, "estatura"]))
frecuencias.estaturas
```

A continuación, ordenamos las frecuencias:

```{r frecuencias_estaturas_ordenadas}
# Lo pasamos a tipo de dato numeric (por defecto esta en tipo factor)
frecuencias.estaturas[, "Estatura"] <- as.numeric(levels(frecuencias.estaturas[, "Estatura"]))
frecuencias.estaturas <- frecuencias.estaturas[order(-frecuencias.estaturas[, "Freq"]),]
frecuencias.estaturas
```

Una vez ordenadas, mediante la función _which_ recuperamos aquellas estaturas cuya frecuencia absoluta corresponda con la frecuencia máxima en el DataFrame. Dado que el máximo corresponde a varias estaturas, la moda resultante será más de un valor:

```{r calculo_moda}
moda <- as.double(frecuencias.estaturas[which(frecuencias.estaturas[, "Freq"] == 
                                        max(frecuencias.estaturas[, "Freq"])), "Estatura"])
moda
```

Por tanto, las estaturas más repetidas son 1.21, 1.22 y 1.28 metros.

A continuación, analizamos las medidas de dispersión con el objetivo de estudiar si los datos se encuentran más o menos concentrados o dispersos:

4. __RANGO__. Para ello, R dispone de una función denominada _range_ que NO calcula el rango, sino que devuelve los valores máximo y mínimo de la muestra. Por tanto, una vez obtenidos ambos valores, se restan mediante la función _diff_:

```{r rango}
# Range devuelve los valores maximo y minimo, NO el rango
range(datos.estatura[, "estatura"])
rango <- diff(range(datos.estatura[, "estatura"]))

rango
```

5. __VARIANZA__. Para el cálculo de la varianza, R dispone de la función _var_ que permite obtener la __cuasi-varianza__, es decir, en lugar de obtener:

$$
\frac{\sum_{i = 1}^{n}({x_i - \bar{x}})^2}{n}
$$
Calcula:

$$
\frac{\sum_{i = 1}^{n}({x_i - \bar{x}})^2}{n - 1}
$$
Por ello, si deseamos obtener la __varianza__ debemos multiplicar el resultado obtenido en la función _var_ por $\frac{(n-1)}{n}$:

```{r varianza}
# Cuasi-varianza
var(datos.estatura[, "estatura"])

# Varianza
varianza <- var(datos.estatura[, "estatura"]) * 
                    ((nrow(datos.estatura) - 1 )/ nrow(datos.estatura))
varianza
```

Sin embargo, la varianza nos devuelve el resultado en las unidades de medida al __cuadrado__, por lo que hay que calcular su raíz cuadrada, es decir, su __desviación típica__, con el objetivo de obtener las mismas unidades que la media.

6. __DESVIACIÓN TÍPICA__. Nuevamente, R dispone de la función _sd_ que obtiene la desviación a partir de la cuasi-varianza, por lo que hay que multiplicar el resultado por $\sqrt{\frac{(n-1)}{n}}$:

```{r desviacion_tipica}
# Desviacion tipica obtenida a partir de la cuasi-varianza
sd(datos.estatura[, "estatura"])

# Desviacion tipica obtenida a partir de la varianza
desv.tipica <- sd(datos.estatura[, "estatura"]) * 
                    sqrt((nrow(datos.estatura) - 1 )/ nrow(datos.estatura))
desv.tipica
```

Analizando el resultado obtenido, podemos comprobar como la dispersión de las medidas con respecto a la media es de unos centímetros de diferencia. No obstante, si realizamos un gráfico de densidad y lo comparamos con el de una distribución normal con la media y desviación típica obtenidas, vemos que muchas de las estaturas no se concentran en torno a la media, sino que observamos una mayor "concentración" de valores en torno a estaturas más bajas y más altas, correspondientes con los valores de la moda obtenidos anteriormente (en torno a 1.21, 1.22 y 1.28 metros). Por el contrario, la densidad al aproximarse a la media (1.25) es menor, por lo que no parece ser __un valor representativo de la muestra__:

```{r grafico_densidad, fig.width=5, fig.height=5, fig.cap="Comparativa entre la densidad estaturas y la distribución normal con la media y desviación típica obtenidas"}
plot(density(datos.estatura[, "estatura"]), type = 'l', ylim = c(0,15),
     lwd = 2, xlab = "Estatura (metros)", ylab = "Densidad",
     main = "Densidad estaturas - distribucion normal")
curve(dnorm(x, mean = media.aritmetica, sd = sd(datos.estatura[, "estatura"])), 
      col = 'red', lwd = 2, type = 'l', add = TRUE)
legend("topleft", legend = c("Densidad estaturas", "Distribucion normal"), 
       col = c("black", "red"), lty = 1, lwd = 2)
```

Una mejor forma de observar dicha dispersión es mediante un __diagrama de caja y bigotes__, empleando la función _boxplot_ de R:

\newpage

```{r diagrama_caja_bigotes, fig.cap="Diagrama de caja y bigotes de las medidas de la muestra"}
boxplot(datos.estatura[, "estatura"], 
        las = 1, ylab = "ESTATURAS", horizontal = TRUE)

# Mediante la funcion summary mostramos los valores de los cuartiles
summary(datos.estatura[, "estatura"])
```

Como podemos observar a partir del diagrama anterior, la parte izquierda del gráfico es significativamente mayor que la de la derecha, es decir,  __las estaturas comprendidas entre el 25 y el 50 % de la muestra (1.22 y 1.26 metros) están mucho más dispersas con respecto a la mediana que las medidas situadas entre 1.26 y 1.28 metros (50 y 75 %)__. Por otro lado, la amplitud de cada "bigote" es la misma (0.02), por lo que ambos extremos presentan la misma concentración:

$$
\text{1er cuartil} - \text{Minimo} = 1.22 -  1.20 = 0.02
$$
$$
\text{Maximo} - \text{3er cuartil} = 1.30 -  1.28 = 0.02
$$

Además, el __rango intercuartílico__ es $Q_3 - Q_1 = 0.06$, es decir, el 50 % de las estaturas de la muestra están comprendidas entre 1.22 y 1.28 metros. Por otra parte, __no se han detectado valores atípicos fuera del rango__.

7. __COEFICIENTE DE VARIACIÓN DE PEARSON__. Se calcula como el cociente entre la desviación típica y la media en términos absolutos:

```{r coeficiente_variacion_pearson}
coef.var.pearson <- desv.tipica / abs(media.aritmetica)
coef.var.pearson
```

Por lo general, dicho coeficiente se emplea para comparar el nivel de __dispersión__ entre dos muestras, especialmente cuando vienen expresadas en distintas unidades (lo cual no ocurre con la desviación típica).

## Apartado 2
__Dado el siguiente conjunto de datos, obtener la tabla de correspondencias, con R, agrupando cada variable en cuatro clases o intervalos. Estos deberán ser elegidos por el alumno.__

Como paso previo a la tabla de correspondencias, cargamos las medidas de estatura en un DataFrame con las columnas alumnos, estatura y peso:

```{r datos_estatura_peso}
datos.estatura.peso <- data.frame(alumnos = c("Alumno1", "Alumno2", "Alumno3", "Alumno4",
                       "Alumno5", "Alumno6", "Alumno7", "Alumno8", "Alumno9",
                       "Alumno10", "Alumno11", "Alumno12", "Alumno13", "Alumno14", 
                       "Alumno15", "Alumno16", "Alumno17", "Alumno18", "Alumno19", 
                       "Alumno20", "Alumno21", "Alumno22", "Alumno23", "Alumno24", 
                       "Alumno25", "Alumno26", "Alumno27", "Alumno28", "Alumno29", 
                       "Alumno30"),
                       
                        estatura = c(1.25, 1.28, 1.27, 1.21, 1.22, 1.29, 1.30, 1.24, 1.27,
                        1.29, 1.25, 1.28, 1.27, 1.21, 1.22, 1.29, 1.30, 1.24,
                        1.27, 1.29, 1.25, 1.28, 1.27, 1.21, 1.22, 1.29, 1.30,
                        1.24, 1.27, 1.29),
                       
                        peso = c(32, 33, 31, 34, 32, 31, 34, 32, 32, 35, 31, 35, 34, 
                        33, 33, 31, 35, 32, 31, 33, 33, 32, 34, 34, 35, 31, 34,
                        33, 35, 34))
```


1. __ESTATURA__:
En primer lugar, las columnas de estatura se agruparán en función de los cuartiles, además de los valores máximo y mínimo. De este modo, agrupando los cuartiles por intervalos es posible analizar con mayor rapidez la distribución o dispersión de los alumnos en base a la estatura. Para obtener dichos valores, empleamos la función _quantile_ disponible en R:

```{r table_estatura}
# Estatura
breaks <- as.vector(quantile(datos.estatura.peso[, "estatura"]))
breaks
```

Una vez recuperados, el objetivo es obtener los siguientes intervalos:

$$
\text{Intervalo peso: } [1.21, 1.24) ;[1.24, 1.27); [1.27, 1.29); [1.29, 1.30]
$$

Como podemos observar, la amplitud de los dos primeros intervalos es diferente con respecto a los dos últimos. Esto permite remarcar los dos últimos donde (si recordamos anteriormente del diagrama de caja y bigotes) se concentran el mayor número de muestras, esto es, entre 1.27 y 1.30. Para ello, la función _cut_ de R nos permite dividir el rango de un vector en intervalos de longitud N pasado como parámetro, además de agrupar el número de datos por cada intervalo. Por tanto, el conjunto de intervalos para la estatura queda de la siguiente forma:

```{r intervalo_estatura}
intervalo.estatura <- cut(datos.estatura.peso[, "estatura"], breaks = breaks, 
                          include.lowest = TRUE, right = FALSE)
levels(intervalo.estatura)
```

Caben destacar los parámetros _include.lowest_ y _right_, los cuales permiten, respectivamente, configurar el intervalo para incluir el valor más bajo, además de establecer el intervalo abierto por la derecha (a excepción del último).

2. __PESO__:
A continuación, analizamos las frecuencias de la columna peso:

```{r table_peso}
# Estatura
table(datos.estatura.peso[, "peso"])
```

En este caso, y dado que solo disponemos de 5 valores de peso, salvo el último intervalo el resto estará formado por un único valor. Para ello, desde la función _cut_ basta con pasar como parámetro el número de intervalos a formar (4):

```{r intervalo_peso}
intervalo.peso <- cut(datos.estatura.peso[, "peso"], breaks = 4, 
                      include.lowest = TRUE, right = FALSE)
levels(intervalo.peso)
```

Una vez creados los intervalos, mediante un DataFrame obtenemos la frecuencia de aparición de cada dato, creando una __tabla de correspondencias__:

```{r tabla_correspondencias}
df.intervalos <- data.frame(estatura = intervalo.estatura, 
                            peso = intervalo.peso)
tabla.correspondencias <- table(df.intervalos[, "estatura"], df.intervalos[, "peso"])
tabla.correspondencias
```

De forma adicional, podemos calcular las frecuencias marginales de cada fila y columna, mediante la función _apply_, aplicando a cada fila/columna la función suma ( _sum_ ):

```{r valores_marginales}
tabla.correspondencias <- rbind(tabla.correspondencias, apply(tabla.correspondencias, 2, sum))
tabla.correspondencias <- cbind(tabla.correspondencias, apply(tabla.correspondencias, 1, sum))
tabla.correspondencias
```

Analizando esta última tabla, podemos comprobar como el intervalo de peso con mayor número de alumnos se sitúa entre los 34 y 35 kg, mientras que los intervalos de altura se mueven en torno a 1.27 y 1.30 metros.

# Ejercicio 2
__Considerando, de nuevo, los datos de la primera pregunta del ejercicio anterior, se pide obtener un intervalo de confianza para la diferencia de medias teóricas entre las observaciones de los primeros 15 casos y de los segundos 15 casos.__

Inicialmente, nos encontramos con dos submuestras de alturas de diferentes personas:

```{r submuestras datos.estatura.primeros.ultimos.15, fig.width=5, fig.height=5}
# Ejercicio 2
datos.estatura.primeros.15 <- datos.estatura[1:15,]
datos.estatura.ultimos.15 <- datos.estatura[16:30,]
```

De cara al cálculo del intervalo de confianza, debemos preguntarnos dos cuestiones fundamentales:

1. __¿Los datos están distribuidos normalmente?__
2. __¿La varianza de ambas poblaciones, aunque desconocidas para nosotros, son iguales?__

Para estudiar la normalidad de ambas muestras, una primera aproximación es mediante un gráfico de cuantiles o __Gráfico Q-Q (_Quantile-Quantile_)__, así como un gráfico de densidad, por medio de una función denominada __mostrar_graficos__:

```{r importacion_paquete, results="hide"}
# Importamos el paquete car, el cual contiene la funcion qqPlot
library(car)
```

```{r mostrar_graficos}
mostrar_graficos <- function(datos, columna) {
  par(mfrow = c(1,2))
  qqPlot(datos[, columna], pch=19, las=1, main='QQplot',
        xlab='Cuantiles teoricos', ylab='Cuantiles muestrales',
        envelope=0.95)
  plot(density(datos[, columna]), lwd = 3, col = 'blue',
       xlab = 'Altura (metros)', ylab = 'Densidad',
       main = 'Grafico de densidad')
}
```

Una vez definida la función, mostramos los gráficos correspondientes a ambas submuestras, tanto con el primer subconjunto:

```{r mostrar_graficas_muestra1, fig.width=12, fig.height=6, fig.cap="Grafico Q-Q y de densidad de los primeros 15 datos"}
# Primeros 15 datos
mostrar_graficos(datos.estatura.primeros.15, "estatura")
```

\newpage
Como con el segundo:

```{r mostrar_graficas_muestra2, fig.width=12, fig.height=6, fig.cap="Grafico Q-Q y de densidad de los últimos 15 datos"}
# Primeros 15 datos
mostrar_graficos(datos.estatura.ultimos.15, "estatura")
```

Por un lado, el primer gráfico muestra la comparación entre la distribución de las estaturas de cada submuestra con los __cuantiles de una distribución teórica normal__ (recta azul). En primera instancia, podemos comprobar que existen medidas en ambas submuestras que no se ajustan a la diagonal, es decir, __existen desviaciones con respecto a la recta__. No obstante, ¿Cómo de sustancial es dicho grado de desviación? Para ello, añadimos una __banda de confianza__: líneas discontínuas que representan los límites de confianza, tanto superior como inferior, para los puntos ajustados sobre la recta (por defecto del 95 %). Podemos comprobar como todos los puntos están contenidos dentro de dichos límites, por lo que no puede descartarse que ambas submuestras puedan provenir de una distribución normal.

Sin embargo, el gráfico de densidad tampoco nos aclara la distribución de los datos, aunque bien es cierto que la segunda submuestra presenta una mayor simetría con respecto a la primera. No obstante, como primera impresión no podemos descartar que los datos no provengan de una distribución normal.

Por ello, debemos plantear una alternativa "menos subjetiva" para determinar si los datos provienen de una distribución normal o no. Para ello, planteamos la siguiente hipótesis nula:

$$
\begin{aligned}
H_0: \text{La muestra proviene de una distribución normal}
\\
H_1: \text{La muestra no proviene de una distribución normal}
\end{aligned}
$$

Una posibilidad sería emplear el método de Kolmogorov-Smirnov visto en clase. Sin embargo, dicha técnica de inferencia asume que se conoce la media y desviación típica de la población, parámetros que por supuesto __desconocemos__. Por ello, R dispone de una función específica denominada _shapiro.test_, basada en la técnica Shapiro-Wilk que permite contrastar la normalidad de los datos sin necesidad de conocer los parámetros poblacionales, adecuada además cuando la muestra es pequeña. Dicha técnica se basa en el cálculo del estadístico _W_:

$$
W = \frac{(\sum^n_{i=1}{a_i}{x_i})^2}{\sum^n_{i=1}({x_i}-{\bar{x}})^2}
$$

Donde $x_i$ es el i-ésimo valor de la muestra y $a_i$ el coeficiente obtenido a través de la tabla de contraste de Shapiro-Wilks:

```{r shapiro_test}
# Prueba Shapiro-Wilk en R
# Primeros 15 datos
shapiro.test(datos.estatura.primeros.15[, "estatura"])

# Ultimos 15 datos
shapiro.test(datos.estatura.ultimos.15[, "estatura"])
```

Como podemos observar en las salidas anteriores, los p-valores obtenidos en ambas submuestras son superiores al nivel de significación $\alpha = 0.05$ , lo que significa que __estadísticamente no existe evidencia en contra de que ambas submuestras provengan de una distribución normal__. Por ello, con vistas a este apartado asumiremos que los datos se distribuyen normalmente.

Una vez asumida la distribución normal de los datos, de cara al cálculo del intervalo de confianza debemos preguntarnos ¿Cómo son las varianzas de ambas poblaciones? ¿Cómo están relacionadas? Dado que en función de la respuesta a esta pregunta, el intervalo de confianza será diferente. El objetivo de la estadística inferencial es inducir, a partir de las propiedades de la muestra, el comportamiento de la población. No obstante, a menos que obtengamos toda la información (lo cual es poco probable) __NO podemos conocer a la población con exactitud ($\mu, \sigma$)__, pero si un intervalo entre cuyos valores se estima que se encuentra cada uno de estos parámetros.

Por ello, comenzando con la varianza proponemos la siguiente hipótesis nula:

$$
\begin{aligned}
H_0: \sigma_1 = \sigma_2
\\
H_1: \sigma_1 \neq \sigma_2
\end{aligned}
$$

O dicho de otro modo:

$$
\begin{aligned}
H_0: \frac{\sigma_1}{\sigma_2} = 1
\\
H_1: \frac{\sigma_1}{\sigma_2} \neq 1
\end{aligned}
$$

Para ello, emplearemos la prueba F de Fisher con el objetivo de comparar ambas varianzas. Por ello, se sabe que bajo hipótesis nula:

$$
\begin{aligned}
\frac{S_1^2/\sigma_1^2}{S_2^2/\sigma_2^2} \text{ presenta una distribución en el muestreo }F_{n - 1, m - 1} \text{ donde n y m son los tamaños de las muestras} 
\end{aligned}
$$

En base a dicho estadístico de contraste, el objetivo será encontrar dos valores F de Fisher tales que:

$$
\begin{aligned}
P(F_{n-1,m-1,1-\alpha/2} < \frac{S_1^2/\sigma_1^2}{S_2^2/\sigma_2^2} < F_{n-1,m-1,\alpha/2}) = 1 - \alpha
\end{aligned}
$$

Es decir, un intervalo de confianza para el cociente de las varianzas poblacionales con un nivel de confianza 1 - $\alpha$:

$$
\begin{aligned}
(\frac{S_1^2}{S_2^2} \frac{1}{F_{n-1,m-1,\alpha/2}}, \frac{S_1^2}{S_2^2} \frac{1}{F_{n-1,m-1,1-\alpha/2}})
\end{aligned}
$$

Una primera aproximación sería realizar el cálculo del intervalo anterior de forma manual, creando una función específica denominada __contrastar_varianzas__. En este caso, para obtener el valor F de Fisher de la tabla, R dispone de la función _qf_ , la cual devuelve el valor correspondiente en función de los grados de libertad del numerador y denominador (df1 y df2), así como del valor $\alpha$:

```{r contrastar_varianzas}
contrastar_varianzas <- function(x, y, columna, confianza) {
  n.1 <- nrow(x)
  n.2 <- nrow(y)
  # R calcula la cuasi-varianza, por lo que debemos multiplicar
  # la varianza obtenida por (n - 1) / n para obtener la varianza 
  var.1 <- var(x[, columna]) * ((n.1 - 1 )/ n.1)
  var.2 <- var(y[, columna]) * ((n.2 - 1 )/ n.2)
  estadistico.f <- min(var.1, var.2) / max(var.1, var.2)
  lim.inf <- estadistico.f * (1 / qf(1- (1 - confianza) / 2, df1 = n.1 - 1, df2 = n.2 - 1))
  lim.sup <- estadistico.f * (1 / qf((1 - confianza) / 2, df1 = n.1 - 1, df2 = n.2 - 1))
  cat("Estadistico F: ", estadistico.f, ". Intervalo: ",c(lim.inf, lim.sup))
}
```

Por tanto, para un valor de confianza del 95 %:

```{r prueba_contrastar_varianzas}
contrastar_varianzas(datos.estatura.primeros.15, datos.estatura.ultimos.15, "estatura", 0.95)
```

Para comprobar que el resultado obtenido es correcto, R dispone de una función predefinida denominada _var.test_ , empleada para comparar las varianzas de dos poblaciones:

```{r var_test}
var.test(datos.estatura.primeros.15[, "estatura"], datos.estatura.ultimos.15[, "estatura"])
```

Analizando la salida anterior, podemos comprobar cómo el intervalo resultante es prácticamente idéntico al obtenido en la función __contrastar_varianzas__. De dicha salida caben destacar dos aspectos:

1. En primer lugar, el intervalo obtenido comprende desde 0.31 hasta 2.76, aproximadamente, intervalo de confianza en el que está incluido el 1, lo que supone que ambas varianzas pueden ser iguales ($\frac{\sigma_1}{\sigma_2} = 1$).

2. Por otro lado, el p-valor obtenido a partir del estadístico F (0.8863) es significativamente superior al valor $\alpha = 0.05$.

Incluso reduciendo el intervalo de confianza a un 70 %, el valor de uno sigue contenido en el intervalo:

```{r var_test_70}
var.test(datos.estatura.primeros.15[, "estatura"], datos.estatura.ultimos.15[, "estatura"], 
         conf.level = 0.70)
```

Por ello, se concluye que los resultados obtenidos __no muestran evidencia en contra de la homogeneidad de las varianzas (Homocedasticidad). Por tanto, la hipótesis nula $H_0$ se mantiene__. Como conclusión, podemos asumir que ambas varianzas son iguales.

Por tanto, una vez determinada la normalidad de los datos así como la igualdad de las varianzas poblacionales, ya podemos calcular el __intervalo de confianza para la diferencia de medias con varianzas desconocidas pero iguales__, visto en clase:

$$
\begin{aligned}
IC_\alpha (\mu_1 - \mu_2) = \bar{X} - \bar{Y} \pm t_{n_1 + n_2 - 2, \alpha/2} \frac{\sqrt{(n_1S_x^2 + n_2S_y^2)[(1/n_1) + (1/n_2)]}}
                                                                                    {\sqrt{n_1+n_2-2}}
\end{aligned}
$$

Para ello, y en lugar de calcular a mano el intervalo, R dispone nuevamente de una función predefinida: _t.test_, obteniendo con ella el intervalo de confianza. Asumiendo que no existe evidencia en contra de que ambas varianzas ($\sigma$) sean iguales, marcamos el parámetro _var.equal_ a TRUE, dado que por defecto R asume no son iguales (FALSE):

```{r t_test_95}
# Por defecto, conf.level esta a 0.95
x <- t.test(datos.estatura.primeros.15[, "estatura"], datos.estatura.ultimos.15[, "estatura"], 
       var.equal = TRUE, conf.level = 0.95)
x
```

Como podemos observar en el intervalo resultante, abarca tantos valores positivos como negativos, por lo que cabe la posibilidad de que la diferencia entre ambas medias sea positiva, negativa o incluso cero (pues está incluido en el intervalo), lo que puede suponer que la media de ambas poblaciones puedan ser iguales. Por otra parte, entre la información del contraste de hipóstesis se incluye el estadístico T de contraste (1.132), el número de grados de libertad (28), así como el p-valor obtenido: 0.2672, el cual es superior al nivel de significación $\alpha = 0.05$. Esto último supone que no podemos rechazar la hipótesis nula de que ambas medias sean idénticas:

$$
\begin{aligned}
\text{Intervalo de confianza (95 \%) para la diferencia de medias de estatura: } \left[-0.011; 0.037\right]
\end{aligned}
$$

De hecho, _t.test_ considera como hipótesis nula que __la media entre ambas submuestras son iguales__, ya que si nos fijamos en la salida anterior vemos que considera como hipótesis alternativa o $H_1$ justo lo contrario: la diferencia entre ambas medias sea distinto de cero.

Dicho intervalo puede verse de forma gráfica, donde podemos observar que el valor del estadístico T "cae" dentro del intervalo de confianza del 95 %:
```{r salida_t_test,  fig.width=7, fig.height=5, fig.cap="Valor del estadístico T sobre un intervalo de confianza del 95 %"}
# Cargamos para ello dos librerias adicionales
library(moonBook)
library(webr)

plot(x)
```

Sin embargo, ¿Qué ocurriría si reducimos el intervalo de confianza de un 95 a un 80 % (por ejemplo)?

```{r t_test_70}
x <- t.test(datos.estatura.primeros.15[, "estatura"], datos.estatura.ultimos.15[, "estatura"], 
       var.equal = TRUE, conf.level = 0.80)
x
```

En este último intervalo, a un 80 % de confianza, el valor cero __sigue contenido dentro del intervalo__, por lo que podemos seguir asegurando que no existe evidencia en contra de la igualdad entre ambas medias, dado que el cero se encuentra dentro del intervalo, además de que el p-valor es superior al nivel de significancia $\alpha = 0.20$. En resumen, __sólo se rechazaría $H_0$ para tamaños $\alpha$ mayor al p-valor = 0.2672 (por ejemplo, 0.3)__.

$$
\begin{aligned}
\text{Intervalo de confianza (80 \%) para la diferencia de medias de estatura: }\left[-0.002; 0.029\right]
\end{aligned}
$$

```{r salida_t_test_2,  fig.width=7, fig.height=5, fig.cap="Valor del estadístico T sobre un intervalo de confianza del 80 %"}
# Mostramos graficamente el valor del estadistico T
plot(x)
```

Aunque hemos comprobado que el p-valor es superior al valor $\alpha$ en cada caso, de igual modo podemos realizar el contraste de hipóstesis con ambos porcentajes "a mano", planteando $H_0$ y $H_1$:

$$
\begin{aligned}
H_0: \mu_1 = \mu_2
\\
H_1: \mu_1 \neq \mu_2
\end{aligned}
$$

El objetivo es realizar el cálculo del estadístico _T_:

$$
\begin{aligned}
T = \frac{\lvert\bar{X} - \bar{Y}- 0\rvert}{\sqrt{\frac{(n_1S_x^2 + n_2S_y^2)}{n_1+n_2-2}(\frac{1}{n_1} + \frac{1}{n_2})}}
\end{aligned}
$$

Una vez calculado el valor de T, debemos probar que sea estrictamente menor al valor t-student $t_{m+n-2, \alpha/2}$ . Para ello, creamos una función denominada __contrastar_hipotesis__, la cual es una modificación de la función __intervalo_confianza__ que devuelve tanto el valor del estadístico como el valor t-Student:

```{r contrastar_hipotesis}
contrastar_hipotesis <- function(x, y, columna, confianza) {
  n.1 <- nrow(x)
  n.2 <- nrow(y)
  var.1 <- var(x[, columna]) * ((n.1 - 1 )/ n.1)
  var.2 <- var(y[, columna]) * ((n.2 - 1 )/ n.2)
  media.1 <- mean(x[, columna])
  media.2 <- mean(y[, columna])
  s.c <- (n.1 * var.1 + n.2 * var.2) / (n.1 + n.2 - 2)
  estadistico.t <- (media.1 - media.2) / sqrt(s.c *  (1/n.1 + 1/n.2))
  t.student <-  abs(qt(c((1 - confianza) / 2), df = n.1 + n.2 - 2))
  print(data.frame("T" = estadistico.t, "t.Student" = t.student), row.names = FALSE)
}
```

Una vez definida dicha función, realizamos la prueba con las dos submuestras. En función cada una, el nivel de significación será diferente ($\alpha = 0.05$ para un 95 %, $\alpha = 0.20$ para un 80 %):

```{r prueba_contrastar_hipotesis}
# Con un 95 % de confianza
contrastar_hipotesis(datos.estatura.primeros.15, datos.estatura.ultimos.15, "estatura", 0.95)

# Con un 80 % de confianza
contrastar_hipotesis(datos.estatura.primeros.15, datos.estatura.ultimos.15, "estatura", 0.80)
```

Analizando los resultados obtenidos (coincidiendo el estadístico T con el obtenido en la función _t.test_), en ambos casos $T < t_{m+n+2, \alpha/2}$, por lo que __no podemos rechazar $H_0$ tanto para un nivel de significación del 5 % como incluso del 20 %. Por tanto, asumimos que ambas muestras pertenecen a la misma población, es decir, presentan la misma media__.